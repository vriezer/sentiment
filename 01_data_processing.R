# library(mamlr)
# test <- elasticizer(query_string('country:uk && computerCodes.junk:0', random = T), batch_size = 10, max_batch = 1,es_pwd = es_pwd, update = lemma_writer, localhost = F, documents = T, lemma = F, file = './')

library(udpipe) # For NLP parsing
library(dplyr) # For data wrangling
library(stringr) # For dealing with strings
library(readtext) # For loading txt files
we_file <- 'Data/corpus.txt' # Relative path and filename for word embedding input
ud_data <- 'Data/ud_data.Rds' # Relative path and filename for UDPipe output
raw_docs <- readtext('Docs/*.txt') # Load the raw texts
ud_model <- udpipe_download_model(language = 'english-ewt') # Download the applicable UDPipe model

## Apply the UDPipe model
ud <- as.data.frame(udpipe(ud_model, x = raw_docs$text, parser = "default", doc_id = raw_docs$doc_id)) %>%
  filter(upos != 'PUNCT') %>% # Remove unneeded punctuation
  mutate(
    lem_u = str_c(lemma,upos, sep = '_') # concatenate lemma with UPOS tags
  )

## Remove earlier output generated by this script
unlink(we_file)

## Write lemma_UPOS to file 
cat(ud$lem_u, file = we_file, append = T)
saveRDS(ud, file = ud_data)

# # To create your own validation data csv
# val_data <- 'Data/val_data.csv' # Relative path and filename for manual sentiment coding
# # Create data frame with sentences to manually code sentiment per sentence
# sentences <- ud %>%
#   group_by(doc_id, sentence_id) %>%
#   summarise(
#     text = str_c(token, sep = ' ', collapse = ' ')
#   ) %>%
#   ungroup %>%
#   sample_n(20) # Take a random sample 
# write.csv(sentences,val_data)

### Continue now with generating the word embedding model, as described in 02_GloVe.sh
